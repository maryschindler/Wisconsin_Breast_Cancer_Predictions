import pandas as pd

#math/stats imports
import numpy as np
np.random.seed(42)

from scipy.stats import norm, pearsonr, pointbiserialr, ttest_ind

#viz imports
import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.colors import LogNorm

import seaborn as sns

#modeling imports
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc
from sklearn.preprocessing import StandardScaler

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam


import warnings
warnings.filterwarnings('ignore')

import itertools





df = pd.read_csv('./Data/data.csv')
df.head()


df.info()


df.drop(columns = ['Unnamed: 32'], inplace = True)

df['diagnosis'] = df['diagnosis'].map({'B': 0
                                      , 'M': 1
                                      })


len(df.columns)





mean_columns = ['radius_mean', 'texture_mean', 'perimeter_mean'
                , 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean'
                , 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']

se_columns = ['radius_se', 'texture_se', 'perimeter_se', 'area_se'
              , 'smoothness_se', 'compactness_se', 'concavity_se'
              , 'concave points_se', 'symmetry_se', 'fractal_dimension_se']

worst_columns = ['radius_worst', 'texture_worst', 'perimeter_worst'
                 , 'area_worst', 'smoothness_worst', 'compactness_worst'
                 , 'concavity_worst', 'concave points_worst', 'symmetry_worst'
                 , 'fractal_dimension_worst']








def create_histogram_ax(dataframe, column, ax, title = True):
    """
    Creates a single plot with a histogram and a faint boxplot overlay for a specified column in a DataFrame.
    
    Parameters:
    df (pd.DataFrame): The input DataFrame.
    column (str): The column name for which to create the visualizations.
    ax (matplotlib.axes.Axes): The Axes object to draw the plot onto.
    """
    # Extract the data for the specified column
    data = dataframe[column]

    # Create the histogram
    sns.histplot(data, kde=True, alpha=0.7, ax=ax)

    # Calculate median and quartiles for the boxplot overlay
    median = data.median()
    mean = data.mean()

    # Add vertical lines for mean and median
    ax.axvline(mean, color='orange', linestyle='--', label=f'Mean ({mean:.2f})', alpha=0.7)
    ax.axvline(median, color='orange', linestyle='-', label=f'Median ({median:.2f})', alpha=0.7)

    # ax.set_title(column, fontsize=8)
        
    ax.set_xlabel(xlabel=column, fontsize=8)
    ax.set_ylabel(ylabel='Count', fontsize=8)

    # Set tick label sizes
    ax.tick_params(axis='x', labelsize=8)
    ax.tick_params(axis='y', labelsize=8)

    # Add legend
    ax.legend(fontsize=8)

    return


def create_subplot_grid(dataframe, columns, plot_function, suptitle='Histograms:'):
    sns.set_style("darkgrid")
    
    num_cols = 3
    num_rows = -(-len(columns) // num_cols)
    
    subplot_height = 3
    subplot_width = 4
    
    fig_height = subplot_height * num_rows
    fig_width = subplot_width * num_cols
    
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(fig_width, fig_height))
    
    axes = axes.flatten()
    
    for i, (ax, column) in enumerate(zip(axes, columns)):
        if i < len(columns):
            # Call the plot function without the 'title' parameter
            plot_function(dataframe, column, ax)
        else:
            ax.axis('off')

    for ax in axes[len(columns):]:
        ax.remove()

    plt.suptitle(f'{suptitle}', y=1.01)
    plt.tight_layout()
    
    # Close the figure to prevent automatic display
    plt.close(fig)
    
    return fig


all = create_subplot_grid(df, se_columns, create_histogram_ax, suptitle = 'Histograms: All Samples')
all


create_subplot_grid(df[df['diagnosis'] == 0], se_columns, create_histogram_ax, suptitle = 'Histograms: When Diagnosis is Benign')


create_subplot_grid(df[df['diagnosis'] == 1], se_columns, create_histogram_ax, suptitle = 'Histograms: When Diagnosis is Malignant')











def calculate_pearson_and_significance(df, columns, significance_threshold=0.05):
    """
    Calculates Pearson correlation coefficients and creates a binary column indicating significance.
    
    Parameters:
    df (pd.DataFrame): The input DataFrame containing numeric columns.
    significance_threshold (float): The significance threshold for correlation coefficients (default is 0.05).
    
    Returns:
    pd.DataFrame: Binary DataFrame indicating significance of correlation coefficients.
    pd.DataFrame: Correlation matrix.
    """
    # Calculate Pearson correlation coefficients and p-values
    correlation_matrix = df[columns].corr(method=lambda x, y: pearsonr(x, y)[0])
    p_values = df[columns].corr(method=lambda x, y: pearsonr(x, y)[1])

    # Create a binary column indicating significance
    is_significant = p_values < significance_threshold

    # Convert boolean values to 0/1
    binary_df = is_significant.astype(int)
    
    return binary_df, correlation_matrix

binary_df, corr_matrix = calculate_pearson_and_significance(df, columns = mean_columns)


def visualize_correlation_heatmap(corr_matrix, binary_df):
    """
    Visualizes correlation coefficients using a heatmap with masked insignificant cells.
    
    Parameters:
    corr_matrix (pd.DataFrame): Correlation matrix.
    binary_df (pd.DataFrame): Binary DataFrame indicating significance of correlation coefficients.
    """
    sns.set_style("white")

    # Create mask to remove non-significant (0 binary) values and upper triangular mask
    mask = np.triu(binary_df.values, k=1)  # Mask upper triangle (excluding diagonal)
    mask |= (binary_df.values == 0)  # Mask non-significant (0 binary) values
    
    # Set up the heatmap
    plt.figure(figsize=(12, 10))
    
    # Customize the heatmap
    sns.heatmap(corr_matrix
                ,mask = mask
                ,vmin = -1
                ,vmax = 1
                ,cmap = 'coolwarm'
                ,annot = True
                ,annot_kws = {'fontsize': 10}
                ,fmt = ".2f"
                ,square = True
                ,linewidths = .5)

    # Add title
    plt.title('Correlation Heatmap with Masked Insignificant Cells')

    # Show plot
    plt.show()

    return


binary_df, corr_matrix = calculate_pearson_and_significance(df, columns = mean_columns + se_columns + worst_columns)

visualize_correlation_heatmap(corr_matrix, binary_df)
# Why annot not displaying?





def print_non_significant_relationships(binary_df):
    """
    Prints the row and column names of non-significant relationships.
    
    Parameters:
    binary_df (pd.DataFrame): Binary DataFrame indicating significance of correlation coefficients.
    """
    # Iterate through the DataFrame
    for i, row in binary_df.iterrows():
        # Check if all values in the row are equal to 1 (indicating significance)
        if (row == 1).all():
            print(f"None for '{i}'")
        else:
            print(i)
            # Initialize a set to keep track of processed relationships
            processed_relationships = set()
            # Iterate through the row and print column names where the value is 0
            for j, value in row.items():
                # Check if the relationship is not between the same variable (n, n)
                if i != j:
                    # Check if the relationship has not been processed and the value is 0
                    if (i, j) not in processed_relationships and (j, i) not in processed_relationships and value == 0:
                        print(f"Non-significant relationship between '{i}' and '{j}'")
                        # Add the relationship to the processed set
                        processed_relationships.add((i, j))
            print()

# print_non_significant_relationships(binary_df)


def calculate_pointbiserial_and_significance(dataframe, binary_col, cont_col, alpha = 0.05):
    # Iniit list for results
    results = []

    for col in cont_col:
        filtered_df = dataframe[[binary_col, col]].dropna()

        binary_data = filtered_df[binary_col]
        cont_data = filtered_df[col]

        # Checking appropriate amount of datapoints for calculations:
        if len(binary_data) > 1 and len(cont_data) > 1:
            corr, p_value = pointbiserialr(binary_data, cont_data)
    
            # Is result sig?
            signif = 'Yes' if p_value < alpha else 'No'
    
        else:
            corr, p_value, signif = None, None, 'Not enough data'

        results.append({'Feature': col
                        ,'Correlation': corr
                        ,'p_Value': p_value
                        ,'Significant': signif
                       })

    results_df = pd.DataFrame(results)

    return results_df    


corr_df = calculate_pointbiserial_and_significance(df, binary_col='diagnosis', cont_col=mean_columns)
corr_df


def visualize_singlecol_corr_heatmap(corr_dataframe, outcome):
    sorted_df = corr_dataframe.sort_values(by = 'Correlation'
                                           ,ascending = False)

    heatmap_data = sorted_df.set_index('Feature')

    # Setting up figure
    plt.figure(figsize = (8, 10))
    plt.title(f"Features' Correlation to {outcome}")

    sns.heatmap(heatmap_data[['Correlation']]
                ,vmin = -1
                ,vmax = 1
                ,cmap = 'coolwarm'
                ,annot = True
                ,annot_kws = {'fontsize': 10}
                ,fmt = ".2f"
                # ,square = True
                ,linewidths = .5)

    # Adding hatching to non-sig cells
    for y in range(heatmap_data.shape[0]):
        if heatmap_data['Significant'].iloc[y] == 'No':
            plt.gca().add_patch(plt.Rectangle((0, y), 1, 1, fill = False, hatch = 'x', edgecolor = 'yellow', lw = 1))

    return


visualize_singlecol_corr_heatmap(corr_df, 'Diagnosis')


corr_df = calculate_pointbiserial_and_significance(df, binary_col='diagnosis', cont_col=se_columns)
visualize_singlecol_corr_heatmap(corr_df, 'Diagnosis')


corr_df = calculate_pointbiserial_and_significance(df, binary_col='diagnosis', cont_col=worst_columns)
visualize_singlecol_corr_heatmap(corr_df, 'Diagnosis')


corr_df = calculate_pointbiserial_and_significance(df, binary_col='diagnosis', cont_col=mean_columns + se_columns + worst_columns)
visualize_singlecol_corr_heatmap(corr_df, 'Diagnosis')








def calculate_t_test(dataframe, binary_col, cont_col, alpha = 0.05):
    # Init list for results
    results = []

    for col in cont_col:
        filtered_df = dataframe[[binary_col, col]].dropna()

        group0 = filtered_df[filtered_df[binary_col] == 0][col]
        group1 = filtered_df[filtered_df[binary_col] == 1][col]

        # Calculating means:
        mean0 = group0.mean() if len(group0) > 0 else None
        mean1 = group1.mean() if len(group1) > 0 else None

        # T-Tests
        if len(group0) > 1 and len(group1) > 1:
            t_stat, p_value = ttest_ind(group0, group1)

            # Significant?
            signif = 'Yes' if p_value < alpha else 'No'

        else:
            # Setting None if calculation not possible
            t_stat, p_value, signif = None, None, 'Not enough data'


        diff_means = (mean1 - mean0) if (mean0 is not None and mean1 is not None) else None
        
        # Appending results to list, to build results' dataframe
        results.append({'Feature': col
                        ,'T-statistic': t_stat
                        ,'p_Value': p_value
                        ,'Significant': signif
                        ,'Total Observations': len(filtered_df)
                        ,'Benign Observations': len(group0)
                        ,'Malignant Observations': len(group1)
                        ,'Benign Mean': mean0
                        ,'Malignant Mean': mean1
                        ,'Difference Between Means': diff_means
                       })

    results_df = pd.DataFrame(results)

    return results_df    


calculate_t_test(df, 'diagnosis', mean_columns).sort_values(by = ['T-statistic', 'Difference Between Means'])


calculate_t_test(df, 'diagnosis', se_columns).sort_values(by = ['T-statistic', 'Difference Between Means'])


calculate_t_test(df, 'diagnosis', worst_columns).sort_values(by = ['T-statistic', 'Difference Between Means'])


calculate_t_test(df, 'diagnosis', mean_columns + se_columns + worst_columns).sort_values(by = ['T-statistic', 'Difference Between Means'])











print(df['diagnosis'].value_counts())
print()
print(df['diagnosis'].value_counts(normalize = True))








df.drop(columns = ['fractal_dimension_se', 'symmetry_se'
                   ,'texture_se', 'fractal_dimension_mean'
                   ,'smoothness_se']
        ,inplace = True)


len(df.columns)








def single_feature_logistic_regression(df, target_col, feature_cols, plot=True):
    results = []

    for feature in feature_cols:
        X = df[[feature]]
        y = df[target_col]

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        model = LogisticRegression()
        model.fit(X_train, y_train)

        # Predict probabilities for testing set
        y_prob = model.predict_proba(X_test)
        
        # Get coefficient and intercept
        coef = model.coef_[0][0]
        intercept = model.intercept_[0]

        # Calculate odds ratio
        odds_ratio = np.exp(coef)

        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)

        results.append({'Feature': feature, 'Accuracy': accuracy,
                        'Coefficient': coef, 'Intercept': intercept,
                        'Odds Ratio': odds_ratio})

        # Plot the predicted probabilities for testing data along with actual class labels if plot is True
        if plot:
            plt.rcParams.update({'font.size': 8})
            plt.figure(figsize=(6, 4))
            sns.scatterplot(x=X_test.iloc[:, 0], y=y_prob[:, 1], hue=y_test)
            plt.xlabel(feature)
            plt.ylabel('Probability of Malignancy')
            plt.ylim(-0.10, 1.10)
            plt.title(f'Logistic Regression Predictions - {feature}\nAccuracy: {round(accuracy, 4)}')
            legend =  plt.legend(title='Actual Diagnosis', loc='upper left', fontsize = 8)
            plt.setp(legend.get_title(), fontsize=8)
            plt.show()


    return pd.DataFrame(results)


# Example usage:
# Assuming df is your DataFrame with 'diagnosis' as the target column and other features as feature columns
target_col = 'diagnosis'  # Target column name
features = df.drop(columns = ['id', 'diagnosis']).columns
results_df = single_feature_logistic_regression(df, target_col, features, plot = False)


results_df[results_df['Accuracy'] >= 0.90].sort_values(by = 'Accuracy', ascending = False)


temp = results_df[results_df['Accuracy'] >= 0.95].sort_values(by = 'Accuracy', ascending = False)['Feature']

results_df_refined = single_feature_logistic_regression(df, target_col, temp)








def fit_lasso_logistic_regression(df, target_col, feature_cols, alpha=1.0, scale_features=True):
    """
    Fits a Lasso logistic regression model on the given DataFrame.

    Parameters:
        df (DataFrame): The input DataFrame.
        target_col (str): The name of the target column.
        feature_cols (list of str): The list of feature column names.
        alpha (float): Regularization strength. Lower values specify stronger regularization.
        scale_features (bool): Whether to scale features before fitting the model.

    Returns:
        Tuple: A tuple containing X_train, X_test, y_train, y_test, and a dictionary of results.
    """
    X = df[feature_cols]
    y = df[target_col]

    # Splitting the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)

    if scale_features:
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
    else:
        X_train_scaled = X_train
        X_test_scaled = X_test

    model = LogisticRegression(penalty='l1', solver='liblinear', C=alpha)
    model.fit(X_train_scaled, y_train)

    # Calculate probability scores and predicted labels for the test set
    y_score = model.predict_proba(X_test_scaled)[:, 1]
    y_pred = model.predict(X_test_scaled)

    # Evaluate the model's performance on the training set
    train_accuracy = model.score(X_train_scaled, y_train)

    # Evaluate the model's performance on the testing set
    test_accuracy = model.score(X_test_scaled, y_test)

    res_dict = {
        'features': feature_cols,
        'coefficients': model.coef_[0],
        'intercept': model.intercept_[0],
        'training_accuracy': train_accuracy,
        'testing_accuracy': test_accuracy,
        'y_score': y_score,
        'y_pred': y_pred
    }

    return X_train, X_test, y_train, y_test, res_dict

# Example usage:
# Assuming df is your DataFrame with 'diagnosis' as the target column and other features as feature columns
target_col = 'diagnosis'  # Target column name
features = df.drop(columns = ['id', 'diagnosis']).columns
X_train, X_test, y_train, y_test, lasso_results = fit_lasso_logistic_regression(df, target_col,features)


accuracy = accuracy_score(y_test, lasso_results['y_pred'])
accuracy


lasso_results['training_accuracy'], lasso_results['testing_accuracy']


def plot_roc_curve(y_test, y_score):
    fpr, tpr, _ = roc_curve(y_test, y_score)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')
    plt.xlim([-0.05, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.show()

# Example usage:
# Assuming lasso_results is the dictionary returned by fit_lasso_logistic_regression
plot_roc_curve(y_test, lasso_results['y_score'])


#Classification Report
report = classification_report(y_test, lasso_results['y_pred'])
print(report)


def visualize_confusion_matrix(y_true, y_pred):
    sns.set_style("white")

    # Calculate confusion matrix
    conf_matrix = confusion_matrix(y_true, y_pred)

    # Plot confusion matrix
    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
    plt.figure(figsize=(8, 6))
    
    disp.plot(cmap=plt.cm.Blues)
    
    plt.title('Confusion Matrix')
    plt.show()
    return

# Example usage:
# Assuming lasso_results is the dictionary returned by fit_lasso_logistic_regression
visualize_confusion_matrix(y_test, lasso_results['y_pred'])


def plot_odds_ratios(coefficients, feature_names):
    # Calculate odds ratios
    odds_ratios = np.exp(coefficients)

    # Round the odds ratios to two decimal places
    rounded_odds_ratios = [round(x, 2) for x in odds_ratios]

    # Plotting
    fig, ax = plt.subplots(figsize=(10, 8))
    rects = ax.barh(feature_names, odds_ratios)

    # Bar labels
    for rect, label in zip(rects, rounded_odds_ratios):
        width = rect.get_width()
        ax.text(width, rect.get_y() + rect.get_height() / 2, f'{label}', ha='left', va='center', color='grey', fontsize=8)

    # Add a red dashed vertical line at 1
    ax.axvline(x=1, color='red', linestyle='--')
    
    ax.set_xlabel('Odds Ratio')
    ax.set_ylabel('Features')
    ax.set_title('Feature Importance (Odds Ratios)')

    plt.gca().invert_yaxis()
    plt.show()

    return


coefficients = lasso_results['coefficients']
feature_names = lasso_results['features']

# Plot odds ratios
plot_odds_ratios(coefficients, feature_names)


def plot_odds_ratios(coefficients, feature_names):
    # Calculate odds ratios
    odds_ratios = np.exp(coefficients)

    # Round the odds ratios to two decimal places
    rounded_odds_ratios = [round(x, 2) for x in odds_ratios]

    # Sort feature names and odds ratios based on odds ratios
    sorted_data = sorted(zip(feature_names, rounded_odds_ratios), key=lambda x: x[1], reverse=True)
    sorted_feature_names, sorted_rounded_odds_ratios = zip(*sorted_data)

    # # Create colormap with proper normalization
    # norm = LogNorm(vmin=min(sorted_rounded_odds_ratios), vmax=max(sorted_rounded_odds_ratios))
    # cmap = cm.coolwarm

    # Plotting
    fig, ax = plt.subplots(figsize=(10, 8))
    rects = ax.barh(sorted_feature_names, sorted_rounded_odds_ratios)

    # Bar labels
    for rect, label in zip(rects, sorted_rounded_odds_ratios):
        width = rect.get_width()
        ax.text(width, rect.get_y() + rect.get_height() / 2, f'{label}', ha='left', va='center', color='grey', fontsize=8)

    # Add a red dashed vertical line at 1
    ax.axvline(x=1, color='red', linestyle='--')
    
    ax.set_xlabel('Odds Ratio')
    ax.set_ylabel('Features')
    ax.set_title('Feature Importance (Odds Ratios)')

    plt.gca().invert_yaxis()
    plt.show()

    return


coefficients = lasso_results['coefficients']
feature_names = lasso_results['features']

# Plot odds ratios
plot_odds_ratios(coefficients, feature_names)


features = lasso_results['features']
coefficients = lasso_results['coefficients']

# Create a DataFrame
df_coeffs = pd.DataFrame({'Feature': features, 'Coefficient': coefficients})

# Filter out instances where coefficient == 0
df_filtered = df_coeffs[df_coeffs['Coefficient'] != 0]

# Get the features after filtering
filtered_features = df_filtered['Feature'].values


filtered_features


X_train, X_test, y_train, y_test, lasso_results_filtered = fit_lasso_logistic_regression(df, target_col, filtered_features)


lasso_results_filtered['training_accuracy'], lasso_results_filtered['testing_accuracy']


coefficients = lasso_results_filtered['coefficients']
feature_names = lasso_results_filtered['features']

plot_odds_ratios(coefficients, feature_names)



