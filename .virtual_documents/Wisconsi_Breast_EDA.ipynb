import os
import pandas as pd
from IPython.display import Image

#math/stats imports
import numpy as np
from scipy.stats import norm, pearsonr, pointbiserialr

#viz imports
import matplotlib.pyplot as plt
import seaborn as sns

#modeling imports
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc


import warnings
warnings.filterwarnings('ignore')

import itertools





df = pd.read_csv('./Data/data.csv')
df.head()


df.info()


df.drop(columns = ['Unnamed: 32'], inplace = True)


mean_columns = ['radius_mean', 'texture_mean', 'perimeter_mean'
                , 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean'
                , 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']

se_columns = ['radius_se', 'texture_se', 'perimeter_se', 'area_se'
              , 'smoothness_se', 'compactness_se', 'concavity_se'
              , 'concave points_se', 'symmetry_se', 'fractal_dimension_se']

worst_columns = ['radius_worst', 'texture_worst', 'perimeter_worst'
                 , 'area_worst', 'smoothness_worst', 'compactness_worst'
                 , 'concavity_worst', 'concave points_worst', 'symmetry_worst'
                 , 'fractal_dimension_worst']


df.columns








def create_histogram(df, column, title = True):
    """
    Creates a single plot with a histogram and a faint boxplot overlay for a specified column in a DataFrame.
    
    Parameters:
    df (pd.DataFrame): The input DataFrame.
    column (str): The column name for which to create the visualizations.
    """
    # Extract the data for the specified column
    data = df[column]

    # Set up the matplotlib figure
    plt.figure(figsize=(8, 6))

    # Create the histogram
    sns.histplot(data, kde=True, alpha=0.7)

    # Calculate median and quartiles for the boxplot overlay
    median = data.median()
    q1, q3 = data.quantile(0.25), data.quantile(0.75)

    # Add vertical lines for quartiles
    plt.axvline(q1, color='orange', linestyle='-.', label=f'Q1 ({q1:.2f})', alpha=0.7)
    plt.axvline(q3, color='orange', linestyle='-.', label=f'Q3 ({q3:.2f})', alpha=0.7)

    # Add vertical lines for mean and median
    plt.axvline(data.mean(), color='orange', linestyle='--', label=f'Mean ({data.mean():.2f})', alpha=0.7)
    plt.axvline(median, color='orange', linestyle='-', label=f'Median ({median:.2f})', alpha=0.7)

    # Add legend and title
    plt.legend()

    if title == True:
        plt.title(f"Histogram for: {column.replace('_', ' ').title()}")

    else:
        plt.title(column)

    # Display the plot
    plt.show()

# Example usage:
create_histogram(df, 'radius_mean')


def create_histogram_ax(df, column, ax, title = True):
    """
    Creates a single plot with a histogram and a faint boxplot overlay for a specified column in a DataFrame.
    
    Parameters:
    df (pd.DataFrame): The input DataFrame.
    column (str): The column name for which to create the visualizations.
    ax (matplotlib.axes.Axes): The Axes object to draw the plot onto.
    """
    # Extract the data for the specified column
    data = df[column]

    # Create the histogram
    sns.histplot(data, kde=True, alpha=0.7, ax=ax)

    # Calculate median and quartiles for the boxplot overlay
    median = data.median()
    q1, q3 = data.quantile(0.25), data.quantile(0.75)

    # Add vertical lines for quartiles
    ax.axvline(q1, color='orange', linestyle='-.', alpha=0.7)
    ax.axvline(q3, color='orange', linestyle='-.', alpha=0.7)

    # Add vertical lines for mean and median
    ax.axvline(data.mean(), color='orange', linestyle='--', label=f'Mean ({data.mean():.2f})', alpha=0.7)
    ax.axvline(median, color='orange', linestyle='-', label=f'Median ({median:.2f})', alpha=0.7)

    # Set title and labels
    if title == True:
        ax.set_title(f"{column.replace('_', ' ').title()}", fontsize=8)

    else: 
        ax.set_title(column, fontsize=8)
        
    ax.set_xlabel(xlabel=column, fontsize=8)
    ax.set_ylabel(ylabel='Count', fontsize=8)

    # Set tick label sizes
    ax.tick_params(axis='x', labelsize=8)
    ax.tick_params(axis='y', labelsize=8)

    # Add legend
    ax.legend(fontsize=8)

    return


def create_subplot_grid(df, columns, plot_function, title = True):
    """
    Creates a grid of plots for specified columns in a DataFrame using the given plotting function.
    
    Parameters:
    df (pd.DataFrame): The input DataFrame.
    columns (list): List of column names for which to create the visualizations.
    plot_function (callable): A function that takes a DataFrame, a column name, and an Axes object, and creates a plot.
    """
    sns.set_style("darkgrid")
    
    num_cols = 3  # Number of columns in the grid
    num_rows = -(-len(columns) // num_cols)  # Number of rows needed to accommodate all columns
    
    # Set the fixed height and width for each subplot
    subplot_height = 3
    subplot_width = 4
    
    # Calculate the figure height and width based on the number of rows and columns and fixed subplot height and width
    fig_height = subplot_height * num_rows
    fig_width = subplot_width * num_cols
    
    # Create a figure with appropriate size and aspect ratio
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(fig_width, fig_height))
    
    # Flatten the axes array to simplify indexing
    axes = axes.flatten()
    
    # Loop through each column and create plots using the specified plotting function
    for i, (ax, column) in enumerate(zip(axes, columns)):
        if i < len(columns):  # Only create plots for the required number of columns
            # Create plot for the current column using the specified plotting function
            plot_function(df, column, ax, title)
        else:
            # Hide empty subplots
            ax.axis('off')

    # Remove excess subplots
    for ax in axes[len(columns):]:
        ax.remove()

    # suptitle
    plt.suptitle('Histograms:', y = 1.01)

    # Adjust layout
    plt.tight_layout()

    return


create_subplot_grid(df, se_columns, create_histogram_ax, title = False)


def calculate_correlation_significance(df, columns, significance_threshold=0.05):
    """
    Calculates Pearson correlation coefficients and creates a binary column indicating significance.
    
    Parameters:
    df (pd.DataFrame): The input DataFrame containing numeric columns.
    significance_threshold (float): The significance threshold for correlation coefficients (default is 0.05).
    
    Returns:
    pd.DataFrame: Binary DataFrame indicating significance of correlation coefficients.
    pd.DataFrame: Correlation matrix.
    """
    # Calculate Pearson correlation coefficients and p-values
    correlation_matrix = df[columns].corr(method=lambda x, y: pearsonr(x, y)[0])
    p_values = df[columns].corr(method=lambda x, y: pearsonr(x, y)[1])

    # Create a binary column indicating significance
    is_significant = p_values < significance_threshold

    # Convert boolean values to 0/1
    binary_df = is_significant.astype(int)
    
    return binary_df, correlation_matrix

binary_df, corr_matrix = calculate_correlation_significance(df, columns = mean_columns)


corr_matrix


def visualize_correlation_heatmap(corr_matrix, binary_df, annot = True):
    """
    Visualizes correlation coefficients using a heatmap with masked insignificant cells.
    
    Parameters:
    corr_matrix (pd.DataFrame): Correlation matrix.
    binary_df (pd.DataFrame): Binary DataFrame indicating significance of correlation coefficients.
    """
    sns.set_style("white")

    # Create mask to remove non-significant (0 binary) values and upper triangular mask
    mask = np.triu(binary_df.values, k=1)  # Mask upper triangle (excluding diagonal)
    mask |= (binary_df.values == 0)  # Mask non-significant (0 binary) values
    
    # Set up the heatmap
    plt.figure(figsize=(12, 10))
    
    # Customize the heatmap
    sns.heatmap(corr_matrix
                ,mask = mask
                ,vmin = -1
                ,vmax = 1
                ,cmap = 'coolwarm'
                ,annot = annot
                ,annot_kws = {'fontsize': 10}
                ,fmt = ".2f"
                ,square = True
                ,linewidths = .5)

    # Add title
    plt.title('Correlation Heatmap with Masked Insignificant Cells')

    # Show plot
    plt.show()

    return


binary_df, corr_matrix = calculate_correlation_significance(df, columns = mean_columns + se_columns + worst_columns)

visualize_correlation_heatmap(corr_matrix, binary_df, annot = False)


binary_df, corr_matrix = calculate_correlation_significance(df, columns = mean_columns)

visualize_correlation_heatmap(corr_matrix, binary_df, annot = False)


binary_df, corr_matrix = calculate_correlation_significance(df, columns = se_columns)

visualize_correlation_heatmap(corr_matrix, binary_df, annot = False)


binary_df, corr_matrix = calculate_correlation_significance(df, columns = worst_columns)

visualize_correlation_heatmap(corr_matrix, binary_df, annot = False)


def print_non_significant_relationships(binary_df):
    """
    Prints the row and column names of non-significant relationships.
    
    Parameters:
    binary_df (pd.DataFrame): Binary DataFrame indicating significance of correlation coefficients.
    """
    # Iterate through the DataFrame
    for i, row in binary_df.iterrows():
        # Check if all values in the row are equal to 1 (indicating significance)
        if (row == 1).all():
            print(f"None for '{i}'")
        else:
            print(i)
            # Initialize a set to keep track of processed relationships
            processed_relationships = set()
            # Iterate through the row and print column names where the value is 0
            for j, value in row.items():
                # Check if the relationship is not between the same variable (n, n)
                if i != j:
                    # Check if the relationship has not been processed and the value is 0
                    if (i, j) not in processed_relationships and (j, i) not in processed_relationships and value == 0:
                        print(f"Non-significant relationship between '{i}' and '{j}'")
                        # Add the relationship to the processed set
                        processed_relationships.add((i, j))
            print()

# print_non_significant_relationships(binary_df)








df.columns


print(df['diagnosis'].value_counts())
print()
print(df['diagnosis'].value_counts(normalize = True))


df['diagnosis'] = df['diagnosis'].map({'B': 0
                                       ,'M': 1})


# sns.pairplot(df[['diagnosis'] + mean_columns], hue = 'diagnosis');


#pointbiserialr use to make corr heatmap for outcome





def single_feature_logistic_regression(df, target_col, feature_cols, plot=True):
    results = []

    for feature in feature_cols:
        X = df[[feature]]
        y = df[target_col]

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        model = LogisticRegression()
        model.fit(X_train, y_train)

        # Predict probabilities for testing set
        y_prob = model.predict_proba(X_test)
        
        # Get coefficient and intercept
        coef = model.coef_[0][0]
        intercept = model.intercept_[0]

        # Calculate odds ratio
        odds_ratio = np.exp(coef)

        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)

        results.append({'Feature': feature, 'Accuracy': accuracy,
                        'Coefficient': coef, 'Intercept': intercept,
                        'Odds Ratio': odds_ratio})

        # Plot the predicted probabilities for testing data along with actual class labels if plot is True
        if plot:
            plt.rcParams.update({'font.size': 8})
            plt.figure(figsize=(6, 4))
            sns.scatterplot(x=X_test.iloc[:, 0], y=y_prob[:, 1], hue=y_test)
            plt.xlabel(feature)
            plt.ylabel('Probability of Malignancy')
            plt.ylim(-0.10, 1.10)
            plt.title(f'Logistic Regression Predictions - {feature}\nAccuracy: {round(accuracy, 4)}')
            legend =  plt.legend(title='Actual Diagnosis', loc='upper left', fontsize = 8)
            plt.setp(legend.get_title(), fontsize=8)
            plt.show()


    return pd.DataFrame(results)


# Example usage:
# Assuming df is your DataFrame with 'diagnosis' as the target column and other features as feature columns
target_col = 'diagnosis'  # Target column name
results_df = single_feature_logistic_regression(df, target_col, mean_columns + se_columns + worst_columns, plot = False)


results_df.sort_values(by = 'Odds Ratio', ascending = False)


results_df[results_df['Accuracy'] >= 0.90].sort_values(by = 'Accuracy', ascending = False)


temp = results_df[results_df['Accuracy'] >= 0.90].sort_values(by = 'Accuracy', ascending = False)['Feature']

results_df_refined = single_feature_logistic_regression(df, target_col, temp)





def fit_lasso_logistic_regression(df, target_col, feature_cols, alpha=1.0):
    X = df[feature_cols]
    y = df[target_col]

    # Splitting the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)

    model = LogisticRegression(penalty='l1', solver='liblinear', C=1/alpha)  # C is the inverse of regularization strength
    model.fit(X_train, y_train)

    # Calculate probability scores and predicted labels for the test set
    y_score = model.predict_proba(X_test)[:, 1]
    y_pred = model.predict(X_test)

    # Evaluate the model's performance on the training set
    train_accuracy = model.score(X_train, y_train)

    # Evaluate the model's performance on the testing set
    test_accuracy = model.score(X_test, y_test)

    res_dict = {
        'features': feature_cols,
        'coefficients': model.coef_[0],
        'intercept': model.intercept_[0],
        'training_accuracy': train_accuracy,
        'testing_accuracy': test_accuracy,
        'y_score': y_score,
        'y_pred': y_pred,
    }

    return X_train, X_test, y_train, y_test, res_dict

# Example usage:
# Assuming df is your DataFrame with 'diagnosis' as the target column and other features as feature columns
target_col = 'diagnosis'  # Target column name
X_train, X_test, y_train, y_test, lasso_results = fit_lasso_logistic_regression(df, target_col, mean_columns + se_columns + worst_columns)


def plot_roc_curve(y_test, y_score):
    fpr, tpr, _ = roc_curve(y_test, y_score)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')
    plt.xlim([-0.05, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.show()

# Example usage:
# Assuming lasso_results is the dictionary returned by fit_lasso_logistic_regression
plot_roc_curve(y_test, lasso_results['y_score'])


def visualize_confusion_matrix(y_true, y_pred):
    sns.set_style("white")

    # Calculate confusion matrix
    conf_matrix = confusion_matrix(y_true, y_pred)

    # Plot confusion matrix
    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
    plt.figure(figsize=(8, 6))
    disp.plot(cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.show()

# Example usage:
# Assuming lasso_results is the dictionary returned by fit_lasso_logistic_regression
visualize_confusion_matrix(y_test, lasso_results['y_pred'])


import numpy as np
import matplotlib.pyplot as plt

def plot_odds_ratios(coefficients, feature_names):
    # Calculate odds ratios
    odds_ratios = np.exp(coefficients)

    # Round the odds ratios to two decimal places
    rounded_odds_ratios = [round(x, 2) for x in odds_ratios]

    # Plotting
    fig, ax = plt.subplots(figsize=(10, 8))
    rects = ax.barh(feature_names, odds_ratios)

    # Bar labels
    for rect, label in zip(rects, rounded_odds_ratios):
        width = rect.get_width()
        ax.text(width, rect.get_y() + rect.get_height() / 2, f'{label}', ha='left', va='center', color='grey', fontsize=8)

    # Add a red dashed vertical line at 1
    ax.axvline(x=1, color='red', linestyle='--')
    
    ax.set_xlabel('Odds Ratio')
    ax.set_ylabel('Features')
    ax.set_title('Feature Importance (Odds Ratios)')

    plt.gca().invert_yaxis()
    plt.show()

# Example usage:
# Assuming you have already run your logistic regression and obtained the coefficients
coefficients = lasso_results['coefficients']
feature_names = lasso_results['features']

# Plot odds ratios
plot_odds_ratios(coefficients, feature_names)



np.exp(coefficients)


sns.displot(data = df, x = 'radius_mean', hue = 'diagnosis')


sns.displot(data = df, x = 'concavity_worst', hue = 'diagnosis')



